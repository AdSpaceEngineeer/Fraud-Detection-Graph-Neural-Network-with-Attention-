import torch.optim as optim
from sklearn.metrics import accuracy_score, recall_score, f1_score

def train(model, data):
    model.train()
    optimizer.zero_grad()
    logits = model(data.x, data.edge_index)
    loss = F.cross_entropy(logits[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

def evaluate(model, data):
    model.eval()
    with torch.no_grad():
        logits = model(data.x, data.edge_index)
        preds = logits.argmax(dim=1)
    # compute metrics on test set
    test_mask = ~data.train_mask
    y_true = data.y[test_mask].cpu().numpy()
    y_pred = preds[test_mask].cpu().numpy()
    acc = accuracy_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    return acc, rec, f1

# Example: training GAT for 50 epochs
optimizer = optim.Adam(model_gat.parameters(), lr=0.001, weight_decay=5e-4)
for epoch in range(1, 51):
    loss = train(model_gat, data)
    if epoch % 10 == 0:
        acc, rec, f1 = evaluate(model_gat, data)
        print(f'Epoch {epoch:02d}: Loss {loss:.4f}, Test Acc {acc:.4f}, Recall {rec:.4f}, F1 {f1:.4f}')
